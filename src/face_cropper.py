"""Face detection and smoothing utilities."""
from __future__ import annotations

import contextlib
import math
from dataclasses import dataclass
from typing import List, Optional

try:
    import cv2
except ModuleNotFoundError as exc:  # pragma: no cover - better error hint for Windows users
    raise RuntimeError(
        "OpenCV (cv2) ist nicht installiert. Bitte `pip install -r requirements.txt` ausfÃ¼hren."
    ) from exc
import numpy as np

_IMPORT_ERROR: Optional[Exception] = None
try:  # pragma: no cover - optional dependency check
    import mediapipe as mp
except ImportError as exc:  # pragma: no cover
    mp = None
    _IMPORT_ERROR = exc

from .utils import CropBox, clamp, expand_crop_for_circle, max_crop_size


@dataclass(slots=True)
class DetectionResult:
    score: float
    box: CropBox


class ExponentialSmoother:
    """Simple exponential moving average smoother for bounding boxes."""

    def __init__(self, alpha: float = 0.6) -> None:
        self.alpha = alpha
        self._state: Optional[np.ndarray] = None

    def reset(self) -> None:
        self._state = None

    def update(self, box: CropBox) -> CropBox:
        vector = np.array([box.x, box.y, box.size], dtype=np.float32)
        if self._state is None:
            self._state = vector
        else:
            self._state = self.alpha * vector + (1 - self.alpha) * self._state
        return CropBox(float(self._state[0]), float(self._state[1]), float(self._state[2]))


class FaceCropper:
    def __init__(
        self,
        min_face: float = 0.1,
        face_priority: str = "largest",
        smoothing: float = 0.4,
        max_tracking_gap: int = 15,
        max_step_fraction: float = 0.15,
    ) -> None:
        self.min_face = min_face
        self.face_priority = face_priority
        self.smoother = ExponentialSmoother(alpha=smoothing)
        self.max_tracking_gap = max(0, int(max_tracking_gap))
        self.max_step_fraction = max(0.0, float(max_step_fraction))
        self._last_smoothed: Optional[CropBox] = None
        self._lost_frames = 0

        self._use_mediapipe = mp is not None
        self._face_detection: Optional["mp.solutions.face_detection.FaceDetection"] = None
        self._cascade: Optional[cv2.CascadeClassifier] = None
        self._hog: Optional[cv2.HOGDescriptor] = None
        if self._use_mediapipe:
            self._face_detection = mp.solutions.face_detection.FaceDetection(
                model_selection=1, min_detection_confidence=0.4
            )
        else:
            cascade_path = cv2.data.haarcascades + "haarcascade_frontalface_default.xml"
            self._cascade = cv2.CascadeClassifier(cascade_path)
            if self._cascade.empty():  # pragma: no cover - depends on local OpenCV data files
                message = (
                    "mediapipe ist nicht installiert und das OpenCV-Haar-Cascade-Modell fehlt."
                    " Bitte installiere entweder mediapipe oder stelle sicher, dass OpenCV korrekt"
                    " installiert ist."
                )
                if mp is None:
                    raise RuntimeError(message) from _IMPORT_ERROR
                raise RuntimeError(message)

        self._hog = cv2.HOGDescriptor()
        self._hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())
        self._saliency = None
        if hasattr(cv2, "saliency") and hasattr(cv2.saliency, "StaticSaliencyFineGrained_create"):
            with contextlib.suppress(Exception):
                self._saliency = cv2.saliency.StaticSaliencyFineGrained_create()

    def close(self) -> None:
        if self._use_mediapipe and self._face_detection is not None:
            with contextlib.suppress(Exception):
                self._face_detection.close()

    def _mp_box_to_crop(
        self,
        detection: "mp.framework.formats.detection_pb2.Detection",
        width: int,
        height: int,
    ) -> CropBox:
        location = detection.location_data
        relative = location.relative_bounding_box
        w = relative.width * width
        h = relative.height * height
        size = max(w, h)
        size = max(size, self.min_face * min(width, height))
        cx = (relative.xmin + relative.width / 2) * width
        cy = (relative.ymin + relative.height / 2) * height
        x = clamp(cx - size / 2, 0, max(0, width - size))
        y = clamp(cy - size / 2, 0, max(0, height - size))
        size = min(size, width, height)
        base = CropBox(x=x, y=y, size=size)
        return self._circle_aligned_box(base, width, height)

    def _detect_with_mediapipe(self, image: np.ndarray) -> List[DetectionResult]:
        detections: List[DetectionResult] = []
        h, w = image.shape[:2]
        assert self._face_detection is not None
        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = self._face_detection.process(rgb)
        if results.detections:
            for detection in results.detections:
                score = detection.score[0] if detection.score else 0.0
                box = self._mp_box_to_crop(detection, w, h)
                detections.append(DetectionResult(score=score, box=box))
        return detections

    def _detect_with_cascade(self, image: np.ndarray) -> List[DetectionResult]:
        detections: List[DetectionResult] = []
        h, w = image.shape[:2]
        assert self._cascade is not None
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        gray = cv2.equalizeHist(gray)
        faces = self._cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, flags=cv2.CASCADE_SCALE_IMAGE)
        for (x, y, width, height) in faces:
            size = max(width, height)
            size = max(size, self.min_face * min(w, h))
            x_adj = clamp(x + width / 2 - size / 2, 0, max(0, w - size))
            y_adj = clamp(y + height / 2 - size / 2, 0, max(0, h - size))
            base = CropBox(x=x_adj, y=y_adj, size=size)
            detections.append(DetectionResult(score=1.0, box=self._circle_aligned_box(base, w, h)))
        return detections

    def _detect_people(self, image: np.ndarray) -> List[DetectionResult]:
        detections: List[DetectionResult] = []
        if self._hog is None:
            return detections
        h, w = image.shape[:2]
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        rects, weights = self._hog.detectMultiScale(gray, winStride=(8, 8), padding=(8, 8), scale=1.05)
        min_size = self.min_face * min(w, h)
        for (x, y, width, height), score in zip(rects, weights):
            size = max(width, height, min_size)
            cx = x + width / 2
            cy = y + height / 2
            x_adj = clamp(cx - size / 2, 0, max(0, w - size))
            y_adj = clamp(cy - size / 2, 0, max(0, h - size))
            base = CropBox(x=x_adj, y=y_adj, size=size)
            detections.append(
                DetectionResult(score=float(max(0.0, min(score, 1.0))), box=self._circle_aligned_box(base, w, h))
            )
        return detections

    def _detect_saliency(self, image: np.ndarray) -> List[DetectionResult]:
        detections: List[DetectionResult] = []
        h, w = image.shape[:2]
        saliency_map: Optional[np.ndarray] = None
        if self._saliency is not None:
            success, saliency = self._saliency.computeSaliency(image)
            if success and saliency is not None:
                saliency_map = (saliency * 255).astype("uint8")
        if saliency_map is None:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            blurred = cv2.GaussianBlur(gray, (5, 5), 0)
            lap = cv2.Laplacian(blurred, cv2.CV_32F)
            saliency_map = cv2.convertScaleAbs(lap)
        blurred_saliency = cv2.GaussianBlur(saliency_map, (5, 5), 0)
        _threshold, binary = cv2.threshold(
            blurred_saliency, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU
        )
        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, np.ones((7, 7), np.uint8))
        contours, _hierarchy = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if not contours:
            return detections
        contours = sorted(contours, key=cv2.contourArea, reverse=True)
        image_area = float(w * h)
        for contour in contours[:3]:
            area = cv2.contourArea(contour)
            if area < image_area * 0.02:
                continue
            x, y, width, height = cv2.boundingRect(contour)
            size = max(width, height)
            cx = x + width / 2
            cy = y + height / 2
            x_adj = clamp(cx - size / 2, 0, max(0, w - size))
            y_adj = clamp(cy - size / 2, 0, max(0, h - size))
            base = CropBox(x=x_adj, y=y_adj, size=max(size, self.min_face * min(w, h)))
            detections.append(DetectionResult(score=0.35, box=self._circle_aligned_box(base, w, h)))
        return detections

    def _circle_aligned_box(self, box: CropBox, width: int, height: int) -> CropBox:
        expanded = expand_crop_for_circle(box)
        min_size = max(box.size, self.min_face * min(width, height))
        max_size = max_crop_size(width, height)
        size = clamp(expanded.size, min_size, max_size)
        center_x = expanded.x + expanded.size / 2
        center_y = expanded.y + expanded.size / 2
        x = clamp(center_x - size / 2, 0.0, max(0.0, width - size))
        y = clamp(center_y - size / 2, 0.0, max(0.0, height - size))
        return CropBox(x=x, y=y, size=size)

    def _filter_relevant_detections(self, detections: List[DetectionResult]) -> List[DetectionResult]:
        if not detections:
            return []
        max_size = max((det.box.size for det in detections), default=0.0)
        if max_size <= 0:
            return []
        size_threshold = max_size * 0.45
        filtered = [
            det
            for det in detections
            if det.box.size >= size_threshold or det.score >= 0.6
        ]
        if len(filtered) >= 2:
            return sorted(filtered, key=lambda det: det.box.size, reverse=True)[:5]

        sorted_by_size = sorted(detections, key=lambda det: det.box.size, reverse=True)
        if not sorted_by_size:
            return []

        result: List[DetectionResult] = [filtered[0] if filtered else sorted_by_size[0]]
        if len(sorted_by_size) >= 2:
            candidate = sorted_by_size[1]
            if candidate.box.size >= max_size * 0.35 or candidate.score >= 0.55:
                result.append(candidate)
        return result

    def combine_detections(
        self,
        detections: List[DetectionResult],
        width: int,
        height: int,
    ) -> Optional[CropBox]:
        relevant = self._filter_relevant_detections(detections)
        if len(relevant) < 2:
            return None
        min_x = min(det.box.x for det in relevant)
        max_x = max(det.box.x + det.box.size for det in relevant)
        min_y = min(det.box.y for det in relevant)
        max_y = max(det.box.y + det.box.size for det in relevant)
        cover_width = max_x - min_x
        cover_height = max_y - min_y
        min_face_size = self.min_face * min(width, height)
        largest_size = max(det.box.size for det in relevant)
        min_size = max(min_face_size, largest_size)
        max_size = max_crop_size(width, height)
        size = max(cover_width, cover_height, largest_size)
        size = clamp(size * 1.05, min_size, max_size)

        weights = [max(0.1, det.score) * det.box.size for det in relevant]
        total_weight = sum(weights) or float(len(relevant))
        centers_x = [(det.box.x + det.box.size / 2) for det in relevant]
        centers_y = [(det.box.y + det.box.size / 2) for det in relevant]
        weighted_cx = sum(cx * w for cx, w in zip(centers_x, weights)) / total_weight
        weighted_cy = sum(cy * w for cy, w in zip(centers_y, weights)) / total_weight

        min_allowed_x = max_x - size
        max_allowed_x = min_x
        min_allowed_y = max_y - size
        max_allowed_y = min_y

        x = clamp(weighted_cx - size / 2, min_allowed_x, max_allowed_x)
        y = clamp(weighted_cy - size / 2, min_allowed_y, max_allowed_y)

        return CropBox(x=x, y=y, size=size)

    def detect_subjects(self, image: np.ndarray) -> List[DetectionResult]:
        detections: List[DetectionResult] = []
        if self._use_mediapipe:
            detections.extend(self._detect_with_mediapipe(image))
        else:
            detections.extend(self._detect_with_cascade(image))
        best_score = max((det.score for det in detections), default=0.0)
        extra_people: List[DetectionResult] = []
        if best_score < 0.6 or len(detections) < 1:
            extra_people = self._detect_people(image)
            if extra_people:
                detections.extend(extra_people)
        if not detections and extra_people:
            detections = extra_people
        if not detections:
            detections = self._detect_saliency(image)
        return detections

    def select_detection(self, detections: List[DetectionResult], width: int, height: int) -> Optional[DetectionResult]:
        if not detections:
            return None
        if self.face_priority == "center":
            center_x = width / 2
            center_y = height / 2
            return min(
                detections,
                key=lambda det: (det.box.x + det.box.size / 2 - center_x) ** 2
                + (det.box.y + det.box.size / 2 - center_y) ** 2,
            )
        if self.face_priority == "largest" or self.face_priority == "all":
            return max(detections, key=lambda det: det.box.size)
        return detections[0]

    def _axis_spans(
        self,
        detections: List[DetectionResult],
        axis: str,
        total: int,
        window_size: float,
    ) -> List[tuple[float, float, float]]:
        spans: List[tuple[float, float, float]] = []
        if not detections:
            return spans
        max_size = max((d.box.size for d in detections), default=1.0)
        axis_center = total / 2
        for det in detections:
            start = det.box.x if axis == "x" else det.box.y
            end = start + det.box.size
            if self.face_priority == "largest":
                weight = 1.0 + det.box.size / max(1.0, max_size)
            elif self.face_priority == "center":
                det_center = (start + end) / 2
                distance = abs(det_center - axis_center)
                normalized = distance / max(axis_center, 1.0)
                weight = 1.0 + max(0.0, 1.0 - normalized)
            else:  # "all"
                overlap_ratio = min(det.box.size, window_size) / max(window_size, 1.0)
                weight = 1.0 + overlap_ratio
            spans.append((start, end, weight))
        return spans

    def _best_axis_position(
        self,
        spans: List[tuple[float, float, float]],
        window_size: float,
        total: int,
    ) -> float:
        limit = max(0.0, total - window_size)
        if limit <= 0:
            return 0.0
        candidates = {0.0, limit}
        center = clamp(total / 2 - window_size / 2, 0.0, limit)
        candidates.add(center)
        for start, end, _weight in spans:
            candidates.add(clamp(start, 0.0, limit))
            candidates.add(clamp(end - window_size, 0.0, limit))

        def coverage(pos: float) -> float:
            window_end = pos + window_size
            value = 0.0
            for start, end, weight in spans:
                overlap = min(window_end, end) - max(pos, start)
                if overlap > 0:
                    value += overlap * weight
            return value

        best_score = -1.0
        best_pos = center
        for pos in sorted(candidates):
            score = coverage(pos)
            if score > best_score or (score == best_score and abs(pos - center) < abs(best_pos - center)):
                best_score = score
                best_pos = pos
        return clamp(best_pos, 0.0, limit)

    def _position_with_constraints(
        self,
        preferred: float,
        min_edge: float,
        max_edge: float,
        size: float,
        total: int,
    ) -> float:
        limit = max(0.0, total - size)
        coverage_min = max(max_edge - size, 0.0)
        coverage_max = min(min_edge, limit)
        if coverage_min > coverage_max:
            return clamp(preferred, 0.0, limit)
        return clamp(preferred, coverage_min, coverage_max)

    def _crop_from_bounds(
        self,
        min_x: float,
        max_x: float,
        min_y: float,
        max_y: float,
        size: float,
        width: int,
        height: int,
        top_ratio: float,
    ) -> CropBox:
        preferred_x = (min_x + max_x) / 2 - size / 2
        preferred_y = min_y - size * top_ratio
        x = self._position_with_constraints(preferred_x, min_x, max_x, size, width)
        y = self._position_with_constraints(preferred_y, min_y, max_y, size, height)
        return CropBox(x=x, y=y, size=size)

    def plan_motion(
        self,
        detections: List[DetectionResult],
        width: int,
        height: int,
        *,
        body_scale: float = 1.7,
        face_margin: float = 0.12,
        body_top_ratio: float = 0.2,
        face_top_ratio: float = 0.08,
    ) -> Optional[tuple[CropBox, CropBox]]:
        relevant = self._filter_relevant_detections(detections)
        if not relevant:
            return None
        relevant = sorted(relevant, key=lambda det: det.box.size, reverse=True)

        min_x = min(det.box.x for det in relevant)
        max_x = max(det.box.x + det.box.size for det in relevant)
        min_y = min(det.box.y for det in relevant)
        max_y = max(det.box.y + det.box.size for det in relevant)

        min_face_size = self.min_face * min(width, height)
        largest_size = max(det.box.size for det in relevant)
        min_size = max(min_face_size, largest_size)
        max_size = max_crop_size(width, height)

        span_width = max_x - min_x
        span_height = max_y - min_y
        base_span = max(span_width, span_height, largest_size)
        face_size = clamp(base_span * (1.0 + face_margin), min_size, max_size)
        body_size = clamp(face_size * body_scale, min_size, max_size)

        fits_all = span_width <= max_size and span_height <= max_size

        if not fits_all and len(relevant) >= 2:
            primary = relevant[0].box
            secondary = relevant[1].box
            primary_min = max(min_face_size, primary.size)
            secondary_min = max(min_face_size, secondary.size)
            start = self._crop_from_bounds(
                primary.x,
                primary.x + primary.size,
                primary.y,
                primary.y + primary.size,
                clamp(primary.size * body_scale, primary_min, max_size),
                width,
                height,
                body_top_ratio,
            )
            end = self._crop_from_bounds(
                secondary.x,
                secondary.x + secondary.size,
                secondary.y,
                secondary.y + secondary.size,
                clamp(secondary.size * (1.0 + face_margin), secondary_min, max_size),
                width,
                height,
                face_top_ratio,
            )
            return start, end

        start = self._crop_from_bounds(min_x, max_x, min_y, max_y, body_size, width, height, body_top_ratio)
        end = self._crop_from_bounds(min_x, max_x, min_y, max_y, face_size, width, height, face_top_ratio)
        return start, end

    def focus_window(
        self,
        detections: List[DetectionResult],
        width: int,
        height: int,
        window_size: float,
    ) -> Optional[CropBox]:
        if not detections:
            return None
        spans_x = self._axis_spans(detections, "x", width, window_size)
        spans_y = self._axis_spans(detections, "y", height, window_size)
        x = self._best_axis_position(spans_x, window_size, width) if spans_x else clamp((width - window_size) / 2, 0.0, max(0.0, width - window_size))
        y = self._best_axis_position(spans_y, window_size, height) if spans_y else clamp((height - window_size) / 2, 0.0, max(0.0, height - window_size))
        return CropBox(x=x, y=y, size=window_size)

    def track(self, frame: np.ndarray, width: int, height: int, fallback: CropBox) -> CropBox:
        detections = self.detect_subjects(frame)
        window_size = fallback.size
        focus: Optional[CropBox]
        combined = self.combine_detections(detections, width, height)
        if combined is not None:
            window_size = combined.size
            focus = combined
        else:
            target = self.select_detection(detections, width, height)
            if target is not None:
                window_size = target.box.size
                focus = CropBox(target.box.x, target.box.y, target.box.size)
            else:
                focus = self.focus_window(detections, width, height, window_size)
        if focus is None:
            self._lost_frames += 1
            if self._last_smoothed is not None and self._lost_frames <= self.max_tracking_gap:
                window_size = self._last_smoothed.size
                preserved = CropBox(self._last_smoothed.x, self._last_smoothed.y, window_size)
                preserved.x = clamp(preserved.x, 0, max(0, width - window_size))
                preserved.y = clamp(preserved.y, 0, max(0, height - window_size))
                return preserved
            self._last_smoothed = None
            self._lost_frames = 0
            self.smoother.reset()
            return fallback

        self._lost_frames = 0
        smoothed = self.smoother.update(focus)

        if self._last_smoothed is not None and self.max_step_fraction > 0:
            max_step = window_size * self.max_step_fraction
            dx = smoothed.x - self._last_smoothed.x
            dy = smoothed.y - self._last_smoothed.y
            if abs(dx) > max_step:
                smoothed.x = self._last_smoothed.x + math.copysign(max_step, dx)
            if abs(dy) > max_step:
                smoothed.y = self._last_smoothed.y + math.copysign(max_step, dy)

        smoothed.size = window_size
        smoothed.x = clamp(smoothed.x, 0, max(0, width - window_size))
        smoothed.y = clamp(smoothed.y, 0, max(0, height - window_size))
        self._last_smoothed = CropBox(smoothed.x, smoothed.y, smoothed.size)
        return self._last_smoothed
